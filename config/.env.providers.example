# ============================================
# LLM Provider Examples
# Copy the configuration you want to .env
# ============================================

# ============================================
# OpenAI (Default - Recommended)
# ============================================
LLM_ENDPOINT=https://api.openai.com/v1/chat/completions
LLM_API_KEY=sk-your-openai-api-key
LLM_MODEL=gpt-4o-mini
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=500

# ============================================
# Anthropic Claude
# ============================================
# LLM_ENDPOINT=https://api.anthropic.com/v1/messages
# LLM_API_KEY=sk-ant-your-anthropic-api-key
# LLM_MODEL=claude-3-sonnet-20240229
# LLM_TEMPERATURE=0.3
# LLM_MAX_TOKENS=500

# ============================================
# Cohere
# ============================================
# LLM_ENDPOINT=https://api.cohere.ai/v1/chat
# LLM_API_KEY=your-cohere-api-key
# LLM_MODEL=command
# LLM_TEMPERATURE=0.3
# LLM_MAX_TOKENS=500

# ============================================
# Azure OpenAI
# ============================================
# LLM_ENDPOINT=https://your-resource.openai.azure.com/openai/deployments/your-deployment/chat/completions?api-version=2024-02-15-preview
# LLM_API_KEY=your-azure-api-key
# LLM_MODEL=gpt-4
# LLM_TEMPERATURE=0.3
# LLM_MAX_TOKENS=500

# ============================================
# Together AI (Open Source Models)
# ============================================
# LLM_ENDPOINT=https://api.together.xyz/v1/chat/completions
# LLM_API_KEY=your-together-api-key
# LLM_MODEL=mistralai/Mixtral-8x7B-Instruct-v0.1
# LLM_TEMPERATURE=0.3
# LLM_MAX_TOKENS=500

# ============================================
# Groq (Ultra-Fast Inference)
# ============================================
# LLM_ENDPOINT=https://api.groq.com/openai/v1/chat/completions
# LLM_API_KEY=your-groq-api-key
# LLM_MODEL=mixtral-8x7b-32768
# LLM_TEMPERATURE=0.3
# LLM_MAX_TOKENS=500

# ============================================
# Anyscale (Llama, Mistral)
# ============================================
# LLM_ENDPOINT=https://api.endpoints.anyscale.com/v1/chat/completions
# LLM_API_KEY=your-anyscale-api-key
# LLM_MODEL=meta-llama/Llama-2-70b-chat-hf
# LLM_TEMPERATURE=0.3
# LLM_MAX_TOKENS=500

# ============================================
# Perplexity (Search-Augmented)
# ============================================
# LLM_ENDPOINT=https://api.perplexity.ai/chat/completions
# LLM_API_KEY=your-perplexity-api-key
# LLM_MODEL=llama-3-sonar-large-32k-online
# LLM_TEMPERATURE=0.3
# LLM_MAX_TOKENS=500

# ============================================
# Custom/Self-Hosted (Local or Private)
# ============================================
# LLM_ENDPOINT=http://localhost:8000/v1/chat/completions
# LLM_API_KEY=local-api-key-or-empty
# LLM_MODEL=your-model-name
# LLM_TEMPERATURE=0.3
# LLM_MAX_TOKENS=500

# ============================================
# Advanced: Custom Headers (if needed)
# ============================================
# LLM_HEADERS={"X-Custom-Header": "value", "X-Another": "value2"}

# ============================================
# Advanced: Custom Request Template (if needed)
# ============================================
# LLM_REQUEST_TEMPLATE={"model": "${parameters.model}", "messages": ${parameters.messages}}
