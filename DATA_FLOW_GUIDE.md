# Data Flow Guide: From Files to Answers

This guide explains the complete data flow in the Explaino RAG system, showing how documents are processed and how queries find answers in the latent space.

## ğŸ”„ Two-Way Pipeline Overview

The system has **two parallel pipelines** that meet in the **latent space** (vector embeddings):

```
ğŸ“ INDEXING PIPELINE (Offline)          ğŸ” QUERY PIPELINE (Real-time)
Documents â†’ Embeddings                   Question â†’ Embedding
        â†“                                        â†“
    Vector DB â†â”€â”€â”€â”€â”€â”€â”€ LATENT SPACE â”€â”€â”€â”€â”€â”€â†’ Search
                    (768-dimensional)
```

---

## ğŸ“¥ Pipeline 1: Indexing Pipeline (Offline)

This pipeline processes your documents and stores them as vectors.

### Step-by-Step Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Data Ingestion                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“ data/transcripts/          ğŸ“ data/pdfs/
    video_001.json                document.pdf
    video_002.json                guide.pdf
         â†“                             â†“
    [Transcript Ingester]        [PDF Ingester]
         â†“                             â†“
    VideoTranscript              PDFParagraph
    objects                      objects


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Text Chunking                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

VideoTranscript                PDFParagraph
    â†“                             â†“
[Chunking Module]             [Chunking Module]
    â†“                             â†“
TranscriptChunk               PDFChunk
(100 tokens each)             (100 tokens each)
with overlap                  with overlap

Example Video Chunk:
{
  "video_id": "v001",
  "start_timestamp": 10.5,
  "end_timestamp": 25.3,
  "text": "Machine learning is a subset of AI..."
}

Example PDF Chunk:
{
  "pdf_filename": "guide.pdf",
  "page_number": 5,
  "paragraph_index": 2,
  "text": "Neural networks consist of layers..."
}


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: Embedding Generation (ENTERING LATENT SPACE)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Text Chunks
    â†“
[Embedding Engine]
(sentence-transformers/all-mpnet-base-v2)
    â†“
768-dimensional vectors

Example:
Text: "Machine learning is a subset of AI..."
    â†“
Vector: [0.234, -0.567, 0.891, ..., 0.123]  (768 numbers)
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              LATENT SPACE
        (Semantic meaning encoded)


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: Vector Storage                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Embeddings + Metadata
    â†“
[OpenSearch Indexer]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenSearch Vector Database           â”‚
â”‚                                      â”‚
â”‚ rag-video-index:                    â”‚
â”‚   - embedding: [0.234, -0.567, ...] â”‚
â”‚   - video_id: "v001"                â”‚
â”‚   - start_timestamp: 10.5           â”‚
â”‚   - text: "Machine learning..."     â”‚
â”‚                                      â”‚
â”‚ rag-pdf-index:                      â”‚
â”‚   - embedding: [0.891, 0.234, ...]  â”‚
â”‚   - pdf_filename: "guide.pdf"       â”‚
â”‚   - page_number: 5                  â”‚
â”‚   - text: "Neural networks..."      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” Pipeline 2: Query Pipeline (Real-time)

This pipeline processes user questions and finds relevant documents.

### Step-by-Step Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Query Input                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Question:
"What is machine learning?"
    â†“
[Query Processor]


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Query Embedding (ENTERING LATENT SPACE)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Question Text
    â†“
[Same Embedding Engine]
(sentence-transformers/all-mpnet-base-v2)
    â†“
768-dimensional query vector

Example:
Question: "What is machine learning?"
    â†“
Query Vector: [0.245, -0.543, 0.876, ..., 0.134]  (768 numbers)
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    LATENT SPACE
              (Same space as documents!)


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: Vector Search (MEETING IN LATENT SPACE)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Query Vector: [0.245, -0.543, 0.876, ...]
    â†“
[OpenSearch k-NN Search]
    â†“
Compares query vector with ALL document vectors
using cosine similarity:

similarity = (query_vector Â· doc_vector) / (||query|| Ã— ||doc||)

Result: Similarity scores (0.0 to 1.0)

Example Results:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Document                    | Similarity Score â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ "Machine learning is..."    | 0.92 âœ… (High!)  â”‚
â”‚ "Neural networks consist..." | 0.78 âœ…          â”‚
â”‚ "The weather today is..."   | 0.23 âŒ (Low)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: Retrieval & Ranking                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[Retrieval Engine]
    â†“
1. Search rag-video-index (Tier 1)
   - Get top 5 results above threshold (0.5)
   - If found â†’ proceed to LLM
   - If not found or LLM refuses â†’ Tier 2

2. Search rag-pdf-index (Tier 2)
   - Get top 5 results above threshold (0.5)
   - If found â†’ proceed to LLM
   - If not found or LLM refuses â†’ Tier 3

3. Return NoAnswer with knowledge summary (Tier 3)


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: LLM Answer Generation                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Retrieved Context + Question
    â†“
[Response Generator]
    â†“
[OpenSearch RAG Pipeline]
    â†“
[LLM (via OpenSearch ML Connector)]
    â†“
Generated Answer

Example:
Context: "Machine learning is a subset of AI that enables
          computers to learn from data..."
Question: "What is machine learning?"
    â†“
Answer: "Machine learning is a subset of artificial
         intelligence that allows computers to learn
         and improve from experience without being
         explicitly programmed."
```

---

## ğŸ¯ The Latent Space: Where Pipelines Meet

### What is Latent Space?

The **latent space** is a 768-dimensional mathematical space where:

- Each dimension represents a learned semantic feature
- Similar meanings are close together
- Different meanings are far apart

```
Latent Space Visualization (simplified to 2D):

                    "AI"
                     â—
                    / \
                   /   \
    "Machine Learning" "Deep Learning"
           â—               â—
           |               |
           |               |
    "Neural Networks"  "Transformers"
           â—               â—


    Far away:
    "Weather" â—                    â— "Cooking"
```

### How Documents and Queries Meet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        LATENT SPACE                              â”‚
â”‚                     (768 dimensions)                             â”‚
â”‚                                                                  â”‚
â”‚  Document Vectors              Query Vector                     â”‚
â”‚  (from indexing)               (from query)                     â”‚
â”‚                                                                  â”‚
â”‚     Doc1: [0.234, -0.567, ...]                                 â”‚
â”‚     Doc2: [0.891, 0.234, ...]                                  â”‚
â”‚     Doc3: [0.123, -0.789, ...]                                 â”‚
â”‚                                                                  â”‚
â”‚                    â†“                                             â”‚
â”‚                                                                  â”‚
â”‚              COSINE SIMILARITY                                   â”‚
â”‚         (How close are they?)                                    â”‚
â”‚                                                                  â”‚
â”‚                    â†“                                             â”‚
â”‚                                                                  â”‚
â”‚     Query: [0.245, -0.543, ...]                                â”‚
â”‚                                                                  â”‚
â”‚     Similarity Scores:                                           â”‚
â”‚     Doc1 â†” Query: 0.92 âœ… (Very similar!)                      â”‚
â”‚     Doc2 â†” Query: 0.78 âœ… (Similar)                            â”‚
â”‚     Doc3 â†” Query: 0.23 âŒ (Not similar)                        â”‚
â”‚                                                                  â”‚
â”‚     Return: Doc1, Doc2 (above threshold 0.5)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Works

1. **Same Embedding Model**: Both pipelines use the same model

   - Documents: `all-mpnet-base-v2`
   - Queries: `all-mpnet-base-v2` (same!)

2. **Same Vector Space**: Both produce 768-dimensional vectors

   - Documents: 768 numbers
   - Queries: 768 numbers (same space!)

3. **Semantic Similarity**: Similar meanings â†’ similar vectors
   - "What is ML?" â‰ˆ "Machine learning is..."
   - Even with different words!

---

## ğŸ“Š Complete End-to-End Example

### Scenario: User asks "What is machine learning?"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INDEXING PIPELINE (Already completed)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Document: "Machine learning is a subset of AI that enables
              computers to learn from data without explicit
              programming."

2. Chunked: Same (fits in one chunk)

3. Embedded: [0.234, -0.567, 0.891, ..., 0.123] (768 dims)

4. Stored in OpenSearch:
   {
     "embedding": [0.234, -0.567, ...],
     "text": "Machine learning is...",
     "video_id": "intro_to_ml",
     "start_timestamp": 15.2
   }


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QUERY PIPELINE (Real-time)                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. User Question: "What is machine learning?"

2. Embedded: [0.245, -0.543, 0.876, ..., 0.134] (768 dims)
   (Very similar to document vector!)

3. k-NN Search in OpenSearch:
   - Compare query vector with all document vectors
   - Calculate cosine similarity
   - Find: Document has 0.92 similarity âœ…

4. Retrieve Context:
   {
     "text": "Machine learning is a subset of AI...",
     "video_id": "intro_to_ml",
     "start_timestamp": 15.2,
     "score": 0.92
   }

5. Generate Answer with LLM:
   Input to LLM:
   - Context: "Machine learning is a subset of AI..."
   - Question: "What is machine learning?"

   Output from LLM:
   "Machine learning is a subset of artificial intelligence
    that enables computers to learn and improve from experience
    without being explicitly programmed."

6. Return Response:
   {
     "answer_type": "video",
     "video_id": "intro_to_ml",
     "start_timestamp": 15.2,
     "generated_answer": "Machine learning is...",
     "score": 0.92
   }
```

---

## ğŸ”‘ Key Concepts

### 1. Embedding Model

- **Purpose**: Converts text to vectors
- **Model**: `sentence-transformers/all-mpnet-base-v2`
- **Output**: 768-dimensional vectors
- **Property**: Similar meanings â†’ similar vectors

### 2. Latent Space

- **Definition**: Mathematical space where vectors live
- **Dimensions**: 768 (each represents a learned feature)
- **Property**: Semantic similarity = geometric proximity

### 3. Cosine Similarity

- **Formula**: `cos(Î¸) = (A Â· B) / (||A|| Ã— ||B||)`
- **Range**: 0.0 (completely different) to 1.0 (identical)
- **Threshold**: 0.5 (configurable via `RELEVANCE_THRESHOLD`)

### 4. k-NN Search

- **Algorithm**: HNSW (Hierarchical Navigable Small World)
- **Purpose**: Find k nearest neighbors in vector space
- **Speed**: Sub-linear time complexity
- **Result**: Top-k most similar documents

---

## ğŸ¨ Visual Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPLETE DATA FLOW                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INDEXING (Offline)                    QUERY (Real-time)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ Files                              â“ Question
  â†“                                     â†“
ğŸ“ Parse                              ğŸ”¤ Text
  â†“                                     â†“
âœ‚ï¸  Chunk
  â†“
ğŸ§® Embed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  ğŸ§® Embed
  â†“                â”‚                    â†“
ğŸ’¾ Store           â”‚                    â”‚
                   â”‚                    â”‚
                   â†“                    â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     LATENT SPACE            â”‚
              â”‚   (768 dimensions)          â”‚
              â”‚                             â”‚
              â”‚  Document Vectors           â”‚
              â”‚        â†•                    â”‚
              â”‚  Query Vector               â”‚
              â”‚        â†“                    â”‚
              â”‚  Cosine Similarity          â”‚
              â”‚        â†“                    â”‚
              â”‚  Top-k Results              â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
                    ğŸ” Retrieve
                         â†“
                    ğŸ¤– LLM Generate
                         â†“
                    âœ… Answer
```

---

## ğŸ“ˆ Performance Characteristics

### Indexing Pipeline

- **Speed**: ~230 documents/second
- **Embedding**: ~90 embeddings/second (CPU)
- **Storage**: ~1KB per document (vector + metadata)
- **Time**: One-time cost (or when adding new data)

### Query Pipeline

- **Embedding**: ~10ms (single query)
- **k-NN Search**: ~50-100ms (depends on index size)
- **LLM Generation**: ~1-2 seconds
- **Total**: < 2 seconds end-to-end

---

## ğŸ”§ Configuration

### Embedding Settings

```bash
# config/.env
EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2
EMBEDDING_DIMENSION=768
EMBEDDING_PROVIDER=local
```

### Retrieval Settings

```bash
# Minimum similarity score to consider relevant
RELEVANCE_THRESHOLD=0.5

# Number of results to retrieve
MAX_RESULTS=5
```

### Chunking Settings

```bash
# Tokens per chunk
CHUNK_SIZE=100

# Overlap between chunks
CHUNK_OVERLAP=20
```

---

## ğŸ¯ Why This Architecture Works

1. **Semantic Understanding**: Embeddings capture meaning, not just keywords
2. **Fast Retrieval**: Vector search is much faster than full-text search
3. **Scalability**: Can handle millions of documents
4. **Flexibility**: Works with any text (videos, PDFs, web pages)
5. **Accuracy**: LLM generates natural answers from retrieved context

---
